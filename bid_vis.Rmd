---
title: "Desfile de los enanos"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(flexdashboard)
```

Column {data-width=650}
-----------------------------------------------------------------------

```{r,eval=F}
rm(list=ls())
library(dplyr)
library(sparklyr)
conf <- spark_config()
  conf$`sparklyr.shell.driver-memory` <- "3G"
  #conf$spark.memory.fraction <- 0.8 
sc <- spark_connect(master = "local", version = "2.4.3",config = conf)  
bd<-spark_read_csv(sc,path="C:\\Users\\ALVARO\\Desktop\\db_bolivia\\bid\\bid.csv",name="bid")
sp_bd<- bd %>% sdf_sample(0.001) %>% compute("ss")
sp_bd %>% tally()
spark_write_csv(sp_bd,"C:\\Users\\ALVARO\\Desktop\\db_bolivia\\bid\\bidsample.csv")
bid<-sp_bd %>% collect()
write.csv(bid,"C:\\Users\\ALVARO\\Desktop\\db_bolivia\\bid\\bidsample.csv")
spark_disconnect(sc)
```

```{r}
rm(list=ls())
library(dplyr)
library(sparklyr)
ineq<-function(x){
  p1<-quantile(x,c(0.01),na.rm = T)
  p99<-quantile(x,c(0.99),na.rm = T)
  aux<-as.double(p99/p1)
  return(aux)
}
conf <- spark_config()
  conf$`sparklyr.shell.driver-memory` <- "3G"
  #conf$spark.memory.fraction <- 0.8 
sc <- spark_connect(master = "local", version = "2.4.3",config = conf)  
bd<-spark_read_csv(sc,path="C:\\Users\\ALVARO\\Desktop\\db_bolivia\\bid\\bidsample.csv",name="bid")
```

### Chart A

```{eval=F}
library(help=srvyr)
aux<-read.csv("C:\\Users\\ALVARO\\Desktop\\db_bolivia\\bid\\bidsample.csv")
aux<-aux[!is.na(aux$factor_ch),]

sd1<-aux %>% as_survey_design(1,weights = factor_ch)
sd2<-bd %>% as_survey_design(1,weights = factor_ch)

sum(aux$factor_ch)

sd1  %>% summarize(survey_total(factor_ch))

bd %>% summarise(survey_mean(factor_ch))

bd %>% group_by(pais_c,anio_c)%>%  summarise(weighted.mean(aedu_ci,factor_ch,na.rm = T))

bd<-bd %>% mutate(a=aedu_ci*factor_ch)
aux %>% select(pais_c) %>% table()

aux %>% group_by(pais_c,anio_c)%>%  summarise(weighted.mean(aedu_ci,factor_ch,na.rm=T))

aux2<-aux %>% group_by(idh_ch) %>% summarise(sum(nmiembros_ch))
bd %>% select(edad_ci)%>% summarise_all(max)
bd %>% filter(edad_ci>=15) %>% group_by(anio_c,pais_c) %>% summarise(percent_approx(ylm_ci,.01))
bd %>% filter(edad_ci>=15) %>% group_by(anio_c,pais_c) %>% 

tab_tbl %>%
  spark_apply(function(df) {
    #defining the UDF called day_extract_num
    day_extract_num<-function (fecha){
      d<-ymd_hms(fecha)
      Y<-toString(year(d))
      M<-toString(month(d))
      D<-toString(day(d))
      d2<-ymd(paste(Y,M,D,sep="-"))
      return<-as.numeric(d2)
    }

    df %>% mutate(day_num=day_extract_num(time))
  })

```

Column {data-width=350}
-----------------------------------------------------------------------

### Chart B

```{r}
library(ggplot2)
t1<-bd %>% filter(edad_ci>=15) %>% group_by(pais_c,anio_c) %>% summarise(edu=mean(aedu_ci),ylab=mean(ylm_ci)) %>% collect()
ggplot(t1 ,aes(x=anio_c,y=edu)) + geom_line(aes(color=pais_c))
spark_disconnect(sc)
qq<-quantile(aux %>% filter(edad_ci>=15 & condocup_ci=="Ocupado") %>% select(ylm_ci),seq(0,1,0.10),na.rm=T)

ll<-quantile(aux %>% filter(edad_ci>=15 & condocup_ci=="Ocupado") %>% select(aedu_ci),seq(0,1,0.10),na.rm=T)
qq
ll
aux<-aux %>% mutate(qq=cut(ylm_ci,qq),ll=cut(aedu_ci,ll))

plot(table(aux$qq,aux$ll))
plot(table(aux$qq,aux$ll))
```

